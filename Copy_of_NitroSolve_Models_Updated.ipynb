{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLhrhSOsKeF6",
        "outputId": "f76c298f-dc75-4daa-a6c1-4c6f4c430a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Change directory to Google Drive\n",
        "\n",
        "%cd /content/drive/MyDrive/Fall24/Deep_Learning/NitroSolveProject/Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB93gU7TMKjs",
        "outputId": "b11963d0-9f48-4be1-c1bb-38582b74fcbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Fall24/Deep_Learning/NitroSolveProject/Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Access and work with the dataset\n",
        "import os\n",
        "data_dir = \"/content/drive/MyDrive/Fall24/Deep_Learning/NitroSolveProject/Data\"\n",
        "files = os.listdir(data_dir)\n",
        "print(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wozOcldLMSxX",
        "outputId": "58f08018-9de1-4874-be73-86eecfd24c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Test.csv', 'Train.csv', 'Test_cleaned.csv', 'Train_cleaned.csv', 'submission_xgboost_supervised.csv', 'submission_xgboost_supervised1.csv', 'submission_cnn.csv', 'submission_cnn_full.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def PreProcessing(train_data, test_data):\n",
        "    #columns_to_drop = ['ID_Zindi', 'NO2_trop',\"LAT\",\"LON\"]\n",
        "    columns_to_drop = ['NO2_trop',\"LAT\",\"LON\"]\n",
        "    train_data = train_data.drop(columns=columns_to_drop)\n",
        "    test_data = test_data.drop(columns=columns_to_drop)\n",
        "\n",
        "    train_data = train_data.dropna(subset=['GT_NO2'])\n",
        "    train_data = train_data.dropna(thresh=len(train_data.columns) - 3)\n",
        "    test_data = test_data.dropna(thresh=len(train_data.columns) - 3)\n",
        "\n",
        "    train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "    test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "\n",
        "    daily_lst_filled = LST_predict(train_data)\n",
        "    train_data['LST'] = train_data.apply(lambda row: daily_lst_filled.get(row['Date'], row['LST']) if pd.isnull(row['LST']) else row['LST'], axis=1)\n",
        "    test_data['LST'] = test_data.apply(lambda row: daily_lst_filled.get(row['Date'], row['LST']) if pd.isnull(row['LST']) else row['LST'], axis=1)\n",
        "\n",
        "    train_data = train_data.drop_duplicates()\n",
        "    test_data = test_data.drop_duplicates()\n",
        "\n",
        "    _, train_data, test_data = scale(train_data, test_data)\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "\n",
        "def scale(train_data,test_data):\n",
        "    numeric_columns = [\"Precipitation\",\"LST\",\"AAI\",\"CloudFraction\",\"NO2_strat\",\"NO2_total\",\"TropopausePressure\"]\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "\n",
        "    scaler = scaler.fit(train_data[numeric_columns])\n",
        "    train_data[numeric_columns] = scaler.transform(train_data[numeric_columns])\n",
        "    test_data[numeric_columns] = scaler.transform(test_data[numeric_columns])\n",
        "\n",
        "    return scaler,train_data,test_data\n",
        "\n",
        "def LST_predict(train_data):\n",
        "    lst_data = train_data.dropna(subset=['LST'])\n",
        "    daily_lst = lst_data.groupby('Date')['LST'].mean()\n",
        "\n",
        "    daily_lst = daily_lst.asfreq('D')\n",
        "    daily_lst_interpolated = daily_lst.interpolate()\n",
        "\n",
        "    arima_model = ARIMA(daily_lst_interpolated, order=(1, 1, 1))\n",
        "    arima_fit = arima_model.fit()\n",
        "\n",
        "    daily_lst_filled = daily_lst.copy()\n",
        "    daily_lst_filled[daily_lst.isnull()] = arima_fit.predict()[daily_lst.isnull()]\n",
        "\n",
        "    daily_lst_filled = daily_lst_filled.to_dict()\n",
        "\n",
        "    return daily_lst_filled\n",
        "\n",
        "\n",
        "# main\n",
        "#file_path_Train = 'GEO_AI\\Train.csv'\n",
        "#file_path_Test = 'GEO_AI\\Test.csv'\n",
        "#train_data = pd.read_csv(file_path_Train)\n",
        "#test_data = pd.read_csv(file_path_Test)\n",
        "\n",
        "train_data = pd.read_csv(data_dir + \"/Train.csv\")\n",
        "test_data = pd.read_csv(data_dir + '/Test.csv')\n",
        "\n",
        "train_data_cleaned, test_data_cleaned = PreProcessing(train_data, test_data)\n",
        "\n",
        "print(train_data_cleaned.head())\n",
        "print(test_data_cleaned.head())\n",
        "\n",
        "train_data_cleaned.to_csv(\"Train_cleaned.csv\")\n",
        "test_data_cleaned.to_csv(\"Test_cleaned.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWMAGYdrLSwC",
        "outputId": "7b00209e-93e7-48fa-f86c-cdaa89b0c8d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-60938854eea1>:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
            "<ipython-input-4-60938854eea1>:18: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
            "  warn('Non-stationary starting autoregressive parameters'\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
            "  warn('Non-invertible starting MA parameters found.'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ID_Zindi       Date    ID  Precipitation       LST       AAI  \\\n",
            "0  ID_ENTGC7 2019-01-01  PD01      -1.000000 -0.263243  0.478834   \n",
            "1  ID_8JCCXC 2019-01-01  PD04      -0.934170 -0.263243  0.395847   \n",
            "2  ID_V3136Z 2019-01-01  RO01      -1.000000 -0.263243  0.422682   \n",
            "3  ID_KRVZDJ 2019-01-01  RO02      -0.974067 -0.263243  0.413168   \n",
            "4  ID_PR351A 2019-01-01  RO03      -0.972466 -0.263243  0.368004   \n",
            "\n",
            "   CloudFraction  NO2_strat  NO2_total  TropopausePressure  GT_NO2  \n",
            "0       0.118234  -0.649832  -0.874815           -0.264086    31.0  \n",
            "1       0.738617  -0.646465  -0.865103           -0.263963    42.0  \n",
            "2       0.348321  -0.643098  -0.904727           -0.264520    31.0  \n",
            "3       0.840107  -0.639731  -0.868017           -0.264084    30.0  \n",
            "4       0.494927  -0.639731  -0.875786           -0.264343    58.0  \n",
            "    ID_Zindi       Date     ID  Precipitation       LST       AAI  \\\n",
            "0  ID_2MYNQS 2019-01-01   PD03      -0.929197 -0.263243  0.330621   \n",
            "1  ID_P4U5WU 2019-01-01   TV03      -1.000000 -0.263243  0.353470   \n",
            "2  ID_U4KWPK 2019-01-01  X5561      -1.000000 -0.212432  0.287712   \n",
            "3  ID_QGSNTZ 2019-01-01  X5953      -0.958350 -0.263243  0.452244   \n",
            "4  ID_GHSZ6K 2019-01-01  X6701      -1.000000 -0.263243  0.361984   \n",
            "\n",
            "   CloudFraction  NO2_strat  NO2_total  TropopausePressure  \n",
            "0       0.542912  -0.643098  -0.915313           -0.264186  \n",
            "1      -0.203583  -0.663300  -0.871901           -0.264942  \n",
            "2      -0.692611  -0.680135  -0.822371           -0.265778  \n",
            "3       0.513834  -0.646465  -0.730109           -0.263800  \n",
            "4       0.357715  -0.659933  -0.843737           -0.264081  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load raw train and test data\n",
        "raw_train_data = pd.read_csv(\"Train.csv\")\n",
        "raw_test_data = pd.read_csv(\"Test.csv\")\n",
        "\n",
        "# Combine all IDs from raw train and test data\n",
        "all_ids = pd.concat([raw_train_data[\"ID_Zindi\"], raw_test_data[\"ID_Zindi\"]], ignore_index=True)\n",
        "\n",
        "# Save all IDs to a file for later use\n",
        "all_ids.to_csv(\"all_ids.csv\", index=False, header=[\"ID_Zindi\"])\n",
        "print(f\"All IDs from Train and Test captured and saved to 'all_ids.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1vks8YHTkB9",
        "outputId": "7ff29265-9d0f-4d0e-de70-7f6b1e14c57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All IDs from Train and Test captured and saved to 'all_ids.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load cleaned data\n",
        "train_data = pd.read_csv(\"Train_cleaned.csv\")\n",
        "test_data = pd.read_csv(\"Test_cleaned.csv\")\n",
        "\n",
        "# Convert 'Date' to datetime and create date-based features\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "\n",
        "# Extract date features\n",
        "for df in [train_data, test_data]:\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "# Drop 'Date' column as it's now redundant\n",
        "train_data = train_data.drop(columns=['Date'])\n",
        "test_data = test_data.drop(columns=['Date'])\n",
        "\n",
        "# Preserve 'ID_Zindi' column for submission purposes\n",
        "test_IDs = test_data[\"ID_Zindi\"]\n",
        "\n",
        "# Add placeholder 'GT_NO2' column in test_data for concatenation\n",
        "test_data['GT_NO2'] = None\n",
        "\n",
        "# Concatenate train and test data for consistent lagged/rolling feature calculation\n",
        "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
        "\n",
        "# Generate lagged features\n",
        "for lag in [1, 7, 30]:\n",
        "    combined_data[f'GT_NO2_lag_{lag}'] = combined_data['GT_NO2'].shift(lag)\n",
        "\n",
        "# Generate rolling features directly on the column\n",
        "combined_data['GT_NO2_roll_mean_7'] = combined_data['GT_NO2'].rolling(window=7).mean()\n",
        "combined_data['GT_NO2_roll_mean_30'] = combined_data['GT_NO2'].rolling(window=30).mean()\n",
        "\n",
        "# Separate the data back into training and test sets\n",
        "train_data = combined_data.iloc[:len(train_data)].dropna(subset=[\"GT_NO2\"])  # Drop rows with NaN in target\n",
        "test_data = combined_data.iloc[len(train_data):].reset_index(drop=True)  # Reset index for clean test data\n",
        "\n",
        "# Drop ID columns in the feature set for training and testing\n",
        "train_data = train_data.drop(columns=[\"ID_Zindi\", \"ID\"])\n",
        "test_data = test_data.drop(columns=[\"ID_Zindi\", \"ID\"])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = train_data.drop(columns=[\"GT_NO2\"])  # Drop target from features\n",
        "y = train_data[\"GT_NO2\"]\n",
        "\n",
        "# Ensure test data has consistent columns with training features\n",
        "X_test = test_data[X.columns]\n",
        "\n",
        "# Split training data for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Validate model\n",
        "y_pred = xgb_model.predict(X_val)\n",
        "rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
        "print(f\"XGBoost Validation RMSE: {rmse}\")\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = xgb_model.predict(X_test)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    \"ID_Zindi\": test_IDs,\n",
        "    \"GT_NO2\": test_predictions\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission_xgboost_supervised.csv\", index=False)\n",
        "print(\"XGBoost supervised learning submission file created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC_Q8WGRUHsS",
        "outputId": "b977caa8-6b2a-4b82-c424-667edb6332ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-bc35dfbefa78>:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Validation RMSE: 9.005203930500716\n",
            "XGBoost supervised learning submission file created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load cleaned data\n",
        "train_data = pd.read_csv(\"Train_cleaned.csv\")\n",
        "test_data = pd.read_csv(\"Test_cleaned.csv\")\n",
        "\n",
        "# Convert 'Date' to datetime and create date-based features\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "\n",
        "# Extract date features\n",
        "for df in [train_data, test_data]:\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "# Drop 'Date' column as it's now redundant\n",
        "train_data = train_data.drop(columns=['Date'])\n",
        "test_data = test_data.drop(columns=['Date'])\n",
        "\n",
        "# Preserve 'ID_Zindi' column for submission purposes\n",
        "test_IDs = test_data[\"ID_Zindi\"]\n",
        "\n",
        "# Add placeholder 'GT_NO2' column in test_data for concatenation\n",
        "test_data['GT_NO2'] = None\n",
        "\n",
        "# Concatenate train and test data for consistent lagged/rolling feature calculation\n",
        "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
        "\n",
        "# Generate lagged features for numeric columns (specifically 'GT_NO2')\n",
        "for lag in [1, 7, 30]:\n",
        "    combined_data[f'GT_NO2_lag_{lag}'] = combined_data['GT_NO2'].shift(lag)\n",
        "\n",
        "# Generate rolling mean features directly on numeric data\n",
        "combined_data['GT_NO2_roll_mean_7'] = combined_data['GT_NO2'].rolling(window=7).mean()\n",
        "combined_data['GT_NO2_roll_mean_30'] = combined_data['GT_NO2'].rolling(window=30).mean()\n",
        "\n",
        "# Separate the data back into training and test sets without dropping any rows in test data\n",
        "train_data = combined_data.iloc[:len(train_data)].dropna(subset=[\"GT_NO2\"])  # Drop rows with NaN in target in train data\n",
        "test_data = combined_data.iloc[len(train_data):].reset_index(drop=True)  # Retain all rows in test data\n",
        "\n",
        "# Fill missing values in numeric columns in test data only\n",
        "numeric_cols = test_data.select_dtypes(include=[float, int]).columns\n",
        "test_data[numeric_cols] = test_data[numeric_cols].fillna(test_data[numeric_cols].mean())\n",
        "\n",
        "# Drop ID columns in the feature set for training and testing\n",
        "train_data = train_data.drop(columns=[\"ID_Zindi\", \"ID\"])\n",
        "test_data = test_data.drop(columns=[\"ID_Zindi\", \"ID\"])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = train_data.drop(columns=[\"GT_NO2\"])  # Drop target from features\n",
        "y = train_data[\"GT_NO2\"]\n",
        "\n",
        "# Ensure test data has consistent columns with training features\n",
        "X_test = test_data[X.columns]\n",
        "\n",
        "# Split training data for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Validate model\n",
        "y_pred = xgb_model.predict(X_val)\n",
        "rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
        "print(f\"XGBoost Validation RMSE: {rmse}\")\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = xgb_model.predict(X_test)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    \"ID_Zindi\": test_IDs,\n",
        "    \"GT_NO2\": test_predictions\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission_xgboost_supervised1.csv\", index=False)\n",
        "print(\"XGBoost supervised learning submission file created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r_Q4JjMbl1g",
        "outputId": "550954e3-a14c-46be-af84-074210fe6f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-06a3306a1e4f>:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Validation RMSE: 9.005203930500716\n",
            "XGBoost supervised learning submission file created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load processed data with lagged and rolling features\n",
        "train_data = pd.read_csv(\"Train_cleaned.csv\")\n",
        "test_data = pd.read_csv(\"Test_cleaned.csv\")\n",
        "\n",
        "# Separate 'ID_Zindi' for submission\n",
        "test_IDs = test_data[\"ID_Zindi\"]\n",
        "\n",
        "# Drop unnecessary columns\n",
        "train_data = train_data.drop(columns=[\"ID_Zindi\", \"ID\"], errors='ignore')\n",
        "test_data = test_data.drop(columns=[\"ID_Zindi\", \"ID\"], errors='ignore')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = train_data.drop(columns=[\"GT_NO2\"])\n",
        "y = train_data[\"GT_NO2\"]\n",
        "\n",
        "# Select only numeric columns for scaling\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "X_numeric = X[numeric_features]\n",
        "X_test_numeric = test_data[numeric_features]\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_numeric)\n",
        "X_test_scaled = scaler.transform(X_test_numeric)\n",
        "\n",
        "# Reshape data for CNN (samples, timesteps, features)\n",
        "X_reshaped = X_scaled.reshape(-1, X_scaled.shape[1], 1)\n",
        "X_test_reshaped = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define CNN model\n",
        "cnn_model = models.Sequential([\n",
        "    layers.Conv1D(64, kernel_size=3, activation=\"relu\", input_shape=(X_train.shape[1], 1)),\n",
        "    layers.Conv1D(32, kernel_size=3, activation=\"relu\"),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "cnn_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)\n",
        "\n",
        "# Evaluate on validation data\n",
        "y_pred = cnn_model.predict(X_val).flatten()\n",
        "rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
        "print(f\"CNN Validation RMSE: {rmse}\")\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = cnn_model.predict(X_test_reshaped).flatten()\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    \"ID_Zindi\": test_IDs,\n",
        "    \"GT_NO2\": test_predictions\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission_cnn.csv\", index=False)\n",
        "print(\"CNN submission file 'submission_cnn.csv' created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTjkesoHqjMG",
        "outputId": "c4e08a16-17b4-4fad-be0b-70a135890a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 207.5304 - root_mean_squared_error: 14.0901 - val_loss: 123.7027 - val_root_mean_squared_error: 11.1222\n",
            "Epoch 2/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 125.1338 - root_mean_squared_error: 11.1859 - val_loss: 123.0407 - val_root_mean_squared_error: 11.0924\n",
            "Epoch 3/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 122.7334 - root_mean_squared_error: 11.0753 - val_loss: 120.8861 - val_root_mean_squared_error: 10.9948\n",
            "Epoch 4/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 121.1691 - root_mean_squared_error: 11.0069 - val_loss: 122.1310 - val_root_mean_squared_error: 11.0513\n",
            "Epoch 5/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 124.7695 - root_mean_squared_error: 11.1695 - val_loss: 119.5223 - val_root_mean_squared_error: 10.9326\n",
            "Epoch 6/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 120.4700 - root_mean_squared_error: 10.9753 - val_loss: 119.6788 - val_root_mean_squared_error: 10.9398\n",
            "Epoch 7/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 121.5498 - root_mean_squared_error: 11.0244 - val_loss: 120.2721 - val_root_mean_squared_error: 10.9669\n",
            "Epoch 8/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 122.1033 - root_mean_squared_error: 11.0492 - val_loss: 120.3459 - val_root_mean_squared_error: 10.9702\n",
            "Epoch 9/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 121.0962 - root_mean_squared_error: 11.0038 - val_loss: 118.0893 - val_root_mean_squared_error: 10.8669\n",
            "Epoch 10/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 122.1103 - root_mean_squared_error: 11.0489 - val_loss: 119.2446 - val_root_mean_squared_error: 10.9199\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "CNN Validation RMSE: 10.919914751658325\n",
            "\u001b[1m 55/179\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "CNN submission file 'submission_cnn.csv' created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all captured IDs from Step 1\n",
        "all_ids = pd.read_csv(\"all_ids.csv\")\n",
        "\n",
        "# Load test data and predictions from the model\n",
        "test_data = pd.read_csv(\"Test_cleaned.csv\")\n",
        "submission_predictions = pd.read_csv(\"submission_cnn.csv\")  # Or use predictions directly\n",
        "\n",
        "# Merge all IDs with model predictions\n",
        "submission_full = pd.merge(all_ids, submission_predictions, on=\"ID_Zindi\", how=\"left\")\n",
        "\n",
        "# Fill missing target values with a placeholder (e.g., mean of predictions)\n",
        "mean_prediction = submission_full[\"GT_NO2\"].mean()\n",
        "submission_full[\"GT_NO2\"].fillna(mean_prediction, inplace=True)\n",
        "\n",
        "# Save the final submission file\n",
        "submission_full.to_csv(\"submission_final.csv\", index=False)\n",
        "print(\"Final submission file with all IDs created successfully as 'submission_final.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqKAcggYUOQY",
        "outputId": "af28f66c-eb85-41a4-c0df-fde3ea1a317f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final submission file with all IDs created successfully as 'submission_final.csv'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-d9d30695d287>:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  submission_full[\"GT_NO2\"].fillna(mean_prediction, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load processed data with lagged and rolling features\n",
        "train_data = pd.read_csv(\"Train_cleaned.csv\")\n",
        "test_data = pd.read_csv(\"Test_cleaned.csv\")\n",
        "\n",
        "# Preserve all IDs for submission purposes\n",
        "test_IDs = test_data[[\"ID_Zindi\"]]\n",
        "\n",
        "# Drop unnecessary columns in both train and test sets\n",
        "train_data = train_data.drop(columns=[\"ID_Zindi\", \"ID\"], errors='ignore')\n",
        "test_data = test_data.drop(columns=[\"ID\"], errors='ignore')  # Keep ID_Zindi for submission\n",
        "\n",
        "# Separate features and target variable\n",
        "X = train_data.drop(columns=[\"GT_NO2\"])\n",
        "y = train_data[\"GT_NO2\"]\n",
        "\n",
        "# Select only numeric columns for scaling, ensuring no rows are dropped\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "X_numeric = X[numeric_features]\n",
        "X_test_numeric = test_data[numeric_features]\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_numeric)\n",
        "X_test_scaled = scaler.transform(X_test_numeric)\n",
        "\n",
        "# Reshape data for CNN (samples, timesteps, features)\n",
        "X_reshaped = X_scaled.reshape(-1, X_scaled.shape[1], 1)\n",
        "X_test_reshaped = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define CNN model\n",
        "cnn_model = models.Sequential([\n",
        "    layers.Conv1D(64, kernel_size=3, activation=\"relu\", input_shape=(X_train.shape[1], 1)),\n",
        "    layers.Conv1D(32, kernel_size=3, activation=\"relu\"),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "cnn_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)\n",
        "\n",
        "# Evaluate on validation data\n",
        "y_pred = cnn_model.predict(X_val).flatten()\n",
        "rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
        "print(f\"CNN Validation RMSE: {rmse}\")\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = cnn_model.predict(X_test_reshaped).flatten()\n",
        "\n",
        "# Ensure all original IDs are preserved in the submission file\n",
        "submission = pd.DataFrame({\n",
        "    \"ID_Zindi\": test_IDs[\"ID_Zindi\"],\n",
        "    \"GT_NO2\": test_predictions\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "submission.to_csv(\"submission_cnn.csv\", index=False)\n",
        "print(\"CNN submission file 'submission_cnn.csv' created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnWVHpePrRxw",
        "outputId": "d00dcac0-caba-4238-e695-3aa74ea277c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: 203.6441 - root_mean_squared_error: 13.9931 - val_loss: 124.2527 - val_root_mean_squared_error: 11.1469\n",
            "Epoch 2/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 125.9684 - root_mean_squared_error: 11.2230 - val_loss: 121.7496 - val_root_mean_squared_error: 11.0340\n",
            "Epoch 3/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 126.1584 - root_mean_squared_error: 11.2309 - val_loss: 131.3956 - val_root_mean_squared_error: 11.4628\n",
            "Epoch 4/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 124.4785 - root_mean_squared_error: 11.1566 - val_loss: 119.7261 - val_root_mean_squared_error: 10.9419\n",
            "Epoch 5/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 121.3746 - root_mean_squared_error: 11.0162 - val_loss: 122.1663 - val_root_mean_squared_error: 11.0529\n",
            "Epoch 6/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 120.1475 - root_mean_squared_error: 10.9609 - val_loss: 121.4795 - val_root_mean_squared_error: 11.0218\n",
            "Epoch 7/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 120.2717 - root_mean_squared_error: 10.9660 - val_loss: 120.0485 - val_root_mean_squared_error: 10.9567\n",
            "Epoch 8/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: 121.3993 - root_mean_squared_error: 11.0163 - val_loss: 120.6674 - val_root_mean_squared_error: 10.9849\n",
            "Epoch 9/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - loss: 121.3512 - root_mean_squared_error: 11.0151 - val_loss: 118.2029 - val_root_mean_squared_error: 10.8721\n",
            "Epoch 10/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 119.1176 - root_mean_squared_error: 10.9132 - val_loss: 117.7921 - val_root_mean_squared_error: 10.8532\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "CNN Validation RMSE: 10.853211535219677\n",
            "\u001b[1m 59/179\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "CNN submission file 'submission_cnn.csv' created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_train_ids = train_data[\"ID_Zindi\"].nunique() if \"ID_Zindi\" in train_data.columns else len(train_data)\n",
        "num_test_ids = test_data[\"ID_Zindi\"].nunique() if \"ID_Zindi\" in test_data.columns else len(test_data)\n",
        "num_submission_ids = submission[\"ID_Zindi\"].nunique()\n",
        "\n",
        "# Print the counts\n",
        "print(f\"Number of unique IDs in Train.csv: {num_train_ids}\")\n",
        "print(f\"Number of unique IDs in Test.csv: {num_test_ids}\")\n",
        "print(f\"Number of unique IDs in submission file: {num_submission_ids}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRsPnm6WtAcB",
        "outputId": "c41b8930-1081-4079-a0d4-af7f7abb4378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique IDs in Train.csv: 69933\n",
            "Number of unique IDs in Test.csv: 5708\n",
            "Number of unique IDs in submission file: 5708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load original data with all IDs\n",
        "train_data = pd.read_csv(\"Train_cleaned.csv\")\n",
        "test_data = pd.read_csv(\"Test_cleaned.csv\")\n",
        "\n",
        "# Preserve 'ID_Zindi' for the final submission\n",
        "all_data = pd.concat([train_data, test_data], ignore_index=True)\n",
        "all_IDs = all_data[\"ID_Zindi\"]\n",
        "\n",
        "# Drop unnecessary columns\n",
        "train_data = train_data.drop(columns=[\"ID_Zindi\", \"ID\"], errors='ignore')\n",
        "test_data = test_data.drop(columns=[\"ID_Zindi\", \"ID\"], errors='ignore')\n",
        "\n",
        "# Separate features and target variable in the training data\n",
        "X = train_data.drop(columns=[\"GT_NO2\"])\n",
        "y = train_data[\"GT_NO2\"]\n",
        "\n",
        "# Combine train and test data for feature scaling and model prediction\n",
        "X_all = all_data.drop(columns=[\"GT_NO2\", \"ID_Zindi\", \"ID\"], errors='ignore')\n",
        "\n",
        "# Select only numeric columns for scaling\n",
        "numeric_features = X_all.select_dtypes(include=[np.number]).columns\n",
        "X_numeric_all = X_all[numeric_features]\n",
        "\n",
        "# Standardize the combined data\n",
        "scaler = StandardScaler()\n",
        "X_scaled_all = scaler.fit_transform(X_numeric_all)\n",
        "\n",
        "# Reshape data for CNN (samples, timesteps, features)\n",
        "X_reshaped_all = X_scaled_all.reshape(-1, X_scaled_all.shape[1], 1)\n",
        "\n",
        "# Split original training data for CNN training and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale training and validation data separately for training purposes\n",
        "X_train_scaled = scaler.transform(X_train[numeric_features])\n",
        "X_train_reshaped = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1)\n",
        "X_val_scaled = scaler.transform(X_val[numeric_features])\n",
        "X_val_reshaped = X_val_scaled.reshape(-1, X_val_scaled.shape[1], 1)\n",
        "\n",
        "# Define CNN model\n",
        "cnn_model = models.Sequential([\n",
        "    layers.Conv1D(64, kernel_size=3, activation=\"relu\", input_shape=(X_train_reshaped.shape[1], 1)),\n",
        "    layers.Conv1D(32, kernel_size=3, activation=\"relu\"),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(X_train_reshaped, y_train, epochs=10, validation_data=(X_val_reshaped, y_val), batch_size=32)\n",
        "\n",
        "# Evaluate on validation data\n",
        "y_val_pred = cnn_model.predict(X_val_reshaped).flatten()\n",
        "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
        "print(f\"CNN Validation RMSE: {rmse}\")\n",
        "\n",
        "# Make predictions on the entire dataset\n",
        "all_predictions = cnn_model.predict(X_reshaped_all).flatten()\n",
        "\n",
        "# Fill in GT_NO2 values from the original training data where available, else use predictions\n",
        "all_data[\"GT_NO2_Predicted\"] = all_predictions\n",
        "all_data[\"GT_NO2_Final\"] = all_data[\"GT_NO2\"].combine_first(all_data[\"GT_NO2_Predicted\"])\n",
        "\n",
        "# Prepare final submission\n",
        "submission = pd.DataFrame({\n",
        "    \"ID_Zindi\": all_IDs,\n",
        "    \"GT_NO2\": all_data[\"GT_NO2_Final\"]\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv(\"submission_cnn_full.csv\", index=False)\n",
        "print(\"CNN submission file 'submission_cnn_full.csv' created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lLRcIG_6X5T",
        "outputId": "53264278-0088-46ba-b93b-908b147dfbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 205.5337 - root_mean_squared_error: 14.0443 - val_loss: 124.1147 - val_root_mean_squared_error: 11.1407\n",
            "Epoch 2/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 126.1994 - root_mean_squared_error: 11.2333 - val_loss: 122.4310 - val_root_mean_squared_error: 11.0649\n",
            "Epoch 3/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 125.4602 - root_mean_squared_error: 11.2002 - val_loss: 120.8644 - val_root_mean_squared_error: 10.9938\n",
            "Epoch 4/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 122.9068 - root_mean_squared_error: 11.0854 - val_loss: 120.0137 - val_root_mean_squared_error: 10.9551\n",
            "Epoch 5/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 121.6372 - root_mean_squared_error: 11.0281 - val_loss: 122.2423 - val_root_mean_squared_error: 11.0563\n",
            "Epoch 6/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 120.2392 - root_mean_squared_error: 10.9648 - val_loss: 119.8555 - val_root_mean_squared_error: 10.9479\n",
            "Epoch 7/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 119.5886 - root_mean_squared_error: 10.9350 - val_loss: 121.1283 - val_root_mean_squared_error: 11.0058\n",
            "Epoch 8/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 117.9871 - root_mean_squared_error: 10.8614 - val_loss: 119.0564 - val_root_mean_squared_error: 10.9113\n",
            "Epoch 9/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 119.2997 - root_mean_squared_error: 10.9217 - val_loss: 119.6134 - val_root_mean_squared_error: 10.9368\n",
            "Epoch 10/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 118.5049 - root_mean_squared_error: 10.8847 - val_loss: 120.3067 - val_root_mean_squared_error: 10.9684\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "CNN Validation RMSE: 10.968442406287963\n",
            "\u001b[1m  84/2364\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 1ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2364/2364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
            "CNN submission file 'submission_cnn_full.csv' created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load cleaned data\n",
        "train_data = pd.read_csv(\"Train_cleaned.csv\")\n",
        "test_data = pd.read_csv(\"Test_cleaned.csv\")\n",
        "\n",
        "# Load all captured IDs from Step 1\n",
        "all_ids = pd.read_csv(\"all_ids.csv\")\n",
        "\n",
        "# Convert 'Date' to datetime and create date-based features\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "\n",
        "# Extract date features\n",
        "for df in [train_data, test_data]:\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "# Drop 'Date' column as it's now redundant\n",
        "train_data = train_data.drop(columns=['Date'])\n",
        "test_data = test_data.drop(columns=['Date'])\n",
        "\n",
        "# Preserve 'ID_Zindi' column for submission purposes\n",
        "test_IDs = test_data[\"ID_Zindi\"]\n",
        "\n",
        "# Add placeholder 'GT_NO2' column in test_data for concatenation\n",
        "test_data['GT_NO2'] = None\n",
        "\n",
        "# Concatenate train and test data for consistent lagged/rolling feature calculation\n",
        "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
        "\n",
        "# Generate lagged features\n",
        "for lag in [1, 7, 30]:\n",
        "    combined_data[f'GT_NO2_lag_{lag}'] = combined_data['GT_NO2'].shift(lag)\n",
        "\n",
        "# Generate rolling features directly on the column\n",
        "combined_data['GT_NO2_roll_mean_7'] = combined_data['GT_NO2'].rolling(window=7).mean()\n",
        "combined_data['GT_NO2_roll_mean_30'] = combined_data['GT_NO2'].rolling(window=30).mean()\n",
        "\n",
        "# Separate the data back into training and test sets\n",
        "train_data = combined_data.iloc[:len(train_data)].dropna(subset=[\"GT_NO2\"])  # Drop rows with NaN in target\n",
        "test_data = combined_data.iloc[len(train_data):].reset_index(drop=True)  # Reset index for clean test data\n",
        "\n",
        "# Drop ID columns in the feature set for training and testing\n",
        "train_data = train_data.drop(columns=[\"ID_Zindi\", \"ID\"])\n",
        "test_data = test_data.drop(columns=[\"ID_Zindi\", \"ID\"])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = train_data.drop(columns=[\"GT_NO2\"])  # Drop target from features\n",
        "y = train_data[\"GT_NO2\"]\n",
        "\n",
        "# Ensure test data has consistent columns with training features\n",
        "X_test = test_data[X.columns]\n",
        "\n",
        "# Split training data for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Validate model\n",
        "y_pred = xgb_model.predict(X_val)\n",
        "rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
        "print(f\"XGBoost Validation RMSE: {rmse}\")\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = xgb_model.predict(X_test)\n",
        "\n",
        "# Create an initial submission file with predicted test IDs\n",
        "submission_predictions = pd.DataFrame({\n",
        "    \"ID_Zindi\": test_IDs,\n",
        "    \"GT_NO2\": test_predictions\n",
        "})\n",
        "\n",
        "# Merge with all IDs to ensure all IDs are in the submission\n",
        "submission_full = pd.merge(all_ids, submission_predictions, on=\"ID_Zindi\", how=\"left\")\n",
        "\n",
        "# Fill missing GT_NO2 values with the mean prediction\n",
        "mean_prediction = submission_full[\"GT_NO2\"].mean()\n",
        "submission_full[\"GT_NO2\"].fillna(mean_prediction, inplace=True)\n",
        "\n",
        "# Save the final submission file\n",
        "submission_full.to_csv(\"submission_xgboost_final.csv\", index=False)\n",
        "print(\"Final XGBoost submission file created as 'submission_xgboost_final.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRyKv78O8-pI",
        "outputId": "ef732821-15f0-41be-e4e5-02c371682610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-82edca82aa37>:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_data = pd.concat([train_data, test_data], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Validation RMSE: 9.005203930500716\n",
            "Final XGBoost submission file created as 'submission_xgboost_final.csv'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n",
            "<ipython-input-10-82edca82aa37>:87: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  submission_full[\"GT_NO2\"].fillna(mean_prediction, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load Data\n",
        "train_data = pd.read_csv(\"Train_cleaned.csv\")\n",
        "test_data = pd.read_csv(\"Test_cleaned.csv\")\n",
        "\n",
        "# Load all captured IDs\n",
        "all_ids = pd.read_csv(\"all_ids.csv\")\n",
        "\n",
        "# Convert 'Date' to datetime and create date-based features\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "\n",
        "# Extract date features\n",
        "for df in [train_data, test_data]:\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "# Drop 'Date' column as it's now redundant\n",
        "train_data = train_data.drop(columns=['Date'])\n",
        "test_data = test_data.drop(columns=['Date'])\n",
        "\n",
        "# Preserve 'ID_Zindi' column for submission purposes\n",
        "test_IDs = test_data[\"ID_Zindi\"]\n",
        "\n",
        "# Add placeholder 'GT_NO2' column in test_data for concatenation\n",
        "test_data['GT_NO2'] = None\n",
        "\n",
        "# Concatenate train and test data for consistent lagged/rolling feature calculation\n",
        "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
        "\n",
        "# Generate lagged features\n",
        "for lag in [1, 7, 30]:\n",
        "    combined_data[f'GT_NO2_lag_{lag}'] = combined_data['GT_NO2'].shift(lag)\n",
        "\n",
        "# Generate rolling features directly on the column\n",
        "combined_data['GT_NO2_roll_mean_7'] = combined_data['GT_NO2'].rolling(window=7).mean()\n",
        "combined_data['GT_NO2_roll_mean_30'] = combined_data['GT_NO2'].rolling(window=30).mean()\n",
        "\n",
        "# Separate the data back into training and test sets\n",
        "train_data = combined_data.iloc[:len(train_data)].dropna(subset=[\"GT_NO2\"])  # Drop rows with NaN in target\n",
        "test_data = combined_data.iloc[len(train_data):].reset_index(drop=True)  # Reset index for clean test data\n",
        "\n",
        "# Drop ID columns in the feature set for training and testing\n",
        "train_data = train_data.drop(columns=[\"ID_Zindi\", \"ID\"])\n",
        "test_data = test_data.drop(columns=[\"ID_Zindi\", \"ID\"])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = train_data.drop(columns=[\"GT_NO2\"])  # Drop target from features\n",
        "y = train_data[\"GT_NO2\"]\n",
        "\n",
        "# Ensure test data has consistent columns with training features\n",
        "X_test = test_data[X.columns]\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape data for LSTM (samples, timesteps, features)\n",
        "X_reshaped = X_scaled.reshape(-1, X_scaled.shape[1], 1)\n",
        "X_test_reshaped = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LSTM model\n",
        "lstm_model = models.Sequential([\n",
        "    layers.LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Train the model\n",
        "lstm_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)\n",
        "\n",
        "# Validate the model\n",
        "y_val_pred = lstm_model.predict(X_val).flatten()\n",
        "\n",
        "# Deal with NaN in y_val or y_val_pred\n",
        "if np.isnan(y_val).any() or np.isnan(y_val_pred).any():\n",
        "    print(\"NaN values detected in y_val or y_val_pred. Cleaning up...\")\n",
        "\n",
        "    # Find valid indices (non-NaN)\n",
        "    valid_indices = ~np.isnan(y_val) & ~np.isnan(y_val_pred)\n",
        "\n",
        "    # Filter y_val and y_val_pred\n",
        "    y_val_cleaned = y_val[valid_indices]\n",
        "    y_val_pred_cleaned = y_val_pred[valid_indices]\n",
        "else:\n",
        "    # Use as is if no NaN values\n",
        "    y_val_cleaned = y_val\n",
        "    y_val_pred_cleaned = y_val_pred\n",
        "\n",
        "# Calculate RMSE using the cleaned arrays\n",
        "rmse = mean_squared_error(y_val_cleaned, y_val_pred_cleaned, squared=False)\n",
        "print(f\"LSTM Validation RMSE: {rmse}\")\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = lstm_model.predict(X_test_reshaped).flatten()\n",
        "\n",
        "# Create an initial submission file with predicted test IDs\n",
        "submission_predictions = pd.DataFrame({\n",
        "    \"ID_Zindi\": test_IDs,\n",
        "    \"GT_NO2\": test_predictions\n",
        "})\n",
        "\n",
        "# Merge with all IDs to ensure all IDs are in the submission\n",
        "submission_full = pd.merge(all_ids, submission_predictions, on=\"ID_Zindi\", how=\"left\")\n",
        "\n",
        "# Fill missing GT_NO2 values with the mean prediction\n",
        "mean_prediction = submission_full[\"GT_NO2\"].mean()\n",
        "submission_full[\"GT_NO2\"].fillna(mean_prediction, inplace=True)\n",
        "\n",
        "# Save the final submission file\n",
        "submission_full.to_csv(\"submission_lstm_final.csv\", index=False)\n",
        "print(\"Final LSTM submission file created as 'submission_lstm_final.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "id": "koVBng_UWD5c",
        "outputId": "3884fcb2-8b05-46bd-a2fe-5001d9816408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-84af416390f6>:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
            "Epoch 2/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
            "Epoch 3/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
            "Epoch 4/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
            "Epoch 5/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
            "Epoch 6/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
            "Epoch 7/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
            "Epoch 8/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
            "Epoch 9/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
            "Epoch 10/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "NaN values detected in y_val or y_val_pred. Cleaning up...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-84af416390f6>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Calculate RMSE using the cleaned arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_pred_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"LSTM Validation RMSE: {rmse}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             return root_mean_squared_error(\n\u001b[0m\u001b[1;32m    503\u001b[0m                 \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mroot_mean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \"\"\"\n\u001b[1;32m    580\u001b[0m     output_errors = np.sqrt(\n\u001b[0;32m--> 581\u001b[0;31m         mean_squared_error(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"raw_values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    504\u001b[0m             )\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1088\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = pd.read_csv(\"Train_cleaned.csv\")\n",
        "test_data = pd.read_csv(\"Test_cleaned.csv\")\n",
        "\n",
        "# Convert 'Date' to datetime and create date-based features\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "\n",
        "# Extract date features\n",
        "for df in [train_data, test_data]:\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "# Drop 'Date' column as it's now redundant\n",
        "train_data = train_data.drop(columns=['Date'])\n",
        "test_data = test_data.drop(columns=['Date'])\n",
        "\n",
        "# Remove any rows in train_data with NaN in the target variable\n",
        "train_data = train_data.dropna(subset=[\"GT_NO2\"])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = train_data.drop(columns=[\"GT_NO2\", \"ID_Zindi\", \"ID\"], errors=\"ignore\")\n",
        "y = train_data[\"GT_NO2\"]\n",
        "\n",
        "# Prepare test features, dropping any non-numeric columns\n",
        "X_test = test_data.drop(columns=[\"ID_Zindi\", \"ID\"], errors=\"ignore\")\n",
        "\n",
        "# Check for NaNs in X, y, and X_test\n",
        "print(f\"NaN values in X: {np.isnan(X).sum().sum()}\")\n",
        "print(f\"NaN values in y: {np.isnan(y).sum()}\")\n",
        "print(f\"NaN values in X_test: {np.isnan(X_test).sum().sum()}\")\n",
        "\n",
        "# Ensure X and X_test contain only numeric columns\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "X = X[numeric_features]\n",
        "X_test = X_test[numeric_features]\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape data for LSTM (samples, timesteps, features)\n",
        "X_reshaped = X_scaled.reshape(-1, X_scaled.shape[1], 1)\n",
        "X_test_reshaped = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LSTM model\n",
        "lstm_model = models.Sequential([\n",
        "    layers.LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "    layers.LSTM(32),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Train the model with validation\n",
        "history = lstm_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)\n",
        "\n",
        "# Validate the model with cleaned y_val and y_val_pred (as handled previously)\n",
        "y_val_pred = lstm_model.predict(X_val).flatten()\n",
        "valid_indices = ~np.isnan(y_val) & ~np.isnan(y_val_pred)\n",
        "y_val_cleaned = y_val[valid_indices]\n",
        "y_val_pred_cleaned = y_val_pred[valid_indices]\n",
        "rmse = mean_squared_error(y_val_cleaned, y_val_pred_cleaned, squared=False)\n",
        "print(f\"LSTM Validation RMSE: {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI7Y6j74X0gm",
        "outputId": "7bec4bef-6683-498d-ad46-ca8e6fb0df27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN values in X: 0\n",
            "NaN values in y: 0\n",
            "NaN values in X_test: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - loss: 329.6425 - root_mean_squared_error: 18.0433 - val_loss: 200.3697 - val_root_mean_squared_error: 14.1552\n",
            "Epoch 2/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 194.7017 - root_mean_squared_error: 13.9464 - val_loss: 185.2168 - val_root_mean_squared_error: 13.6094\n",
            "Epoch 3/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 154.2992 - root_mean_squared_error: 12.4203 - val_loss: 135.7366 - val_root_mean_squared_error: 11.6506\n",
            "Epoch 4/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 133.6549 - root_mean_squared_error: 11.5606 - val_loss: 129.0984 - val_root_mean_squared_error: 11.3621\n",
            "Epoch 5/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 124.7368 - root_mean_squared_error: 11.1654 - val_loss: 122.6263 - val_root_mean_squared_error: 11.0737\n",
            "Epoch 6/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - loss: 122.3613 - root_mean_squared_error: 11.0610 - val_loss: 119.7877 - val_root_mean_squared_error: 10.9448\n",
            "Epoch 7/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 123.2882 - root_mean_squared_error: 11.1031 - val_loss: 117.0430 - val_root_mean_squared_error: 10.8186\n",
            "Epoch 8/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - loss: 119.7850 - root_mean_squared_error: 10.9440 - val_loss: 127.0224 - val_root_mean_squared_error: 11.2704\n",
            "Epoch 9/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 116.1846 - root_mean_squared_error: 10.7769 - val_loss: 114.9761 - val_root_mean_squared_error: 10.7227\n",
            "Epoch 10/10\n",
            "\u001b[1m1749/1749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 120.7038 - root_mean_squared_error: 10.9854 - val_loss: 115.6241 - val_root_mean_squared_error: 10.7529\n",
            "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "LSTM Validation RMSE: 10.752861916431549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "test_predictions = lstm_model.predict(X_test_reshaped).flatten()\n",
        "\n",
        "# Create an initial submission file with predicted test IDs\n",
        "submission_predictions = pd.DataFrame({\n",
        "    \"ID_Zindi\": test_data[\"ID_Zindi\"],\n",
        "    \"GT_NO2\": test_predictions\n",
        "})\n",
        "\n",
        "# Load all captured IDs\n",
        "all_ids = pd.read_csv(\"all_ids.csv\")\n",
        "\n",
        "# Merge with all IDs to ensure all IDs are in the submission\n",
        "submission_full = pd.merge(all_ids, submission_predictions, on=\"ID_Zindi\", how=\"left\")\n",
        "\n",
        "# Fill missing GT_NO2 values with the mean prediction\n",
        "mean_prediction = submission_full[\"GT_NO2\"].mean()\n",
        "submission_full[\"GT_NO2\"].fillna(mean_prediction, inplace=True)\n",
        "\n",
        "# Save the final submission file\n",
        "submission_full.to_csv(\"submission_lstm_final.csv\", index=False)\n",
        "print(\"Final LSTM submission file created as 'submission_lstm_final.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBGXk0hweOug",
        "outputId": "66a86158-1b5c-4f80-9216-c990ff162448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-2a3bdb6eaa01>:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  submission_full[\"GT_NO2\"].fillna(mean_prediction, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final LSTM submission file created as 'submission_lstm_final.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3D CNN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = pd.read_csv(\"Train_cleaned.csv\")\n",
        "test_data = pd.read_csv(\"Test_cleaned.csv\")\n",
        "\n",
        "# Convert 'Date' to datetime and create date-based features\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "\n",
        "# Extract date features\n",
        "for df in [train_data, test_data]:\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "# Drop 'Date' column as it's now redundant\n",
        "train_data = train_data.drop(columns=['Date'])\n",
        "test_data = test_data.drop(columns=['Date'])\n",
        "\n",
        "# Remove any rows in train_data with NaN in the target variable\n",
        "train_data = train_data.dropna(subset=[\"GT_NO2\"])\n",
        "\n",
        "# Separate features and target variable\n",
        "X = train_data.drop(columns=[\"GT_NO2\", \"ID_Zindi\", \"ID\"], errors=\"ignore\")\n",
        "y = train_data[\"GT_NO2\"]\n",
        "\n",
        "# Prepare test features, dropping any non-numeric columns\n",
        "X_test = test_data.drop(columns=[\"ID_Zindi\", \"ID\"], errors=\"ignore\")\n",
        "\n",
        "# Ensure X and X_test contain only numeric columns\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "X = X[numeric_features]\n",
        "X_test = X_test[numeric_features]\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape data for 3D CNN (samples, timesteps, features)\n",
        "timesteps = 10  # Example: use 10 timesteps\n",
        "X_reshaped = X_scaled.reshape(-1, timesteps, X_scaled.shape[1] // timesteps)\n",
        "X_test_reshaped = X_test_scaled.reshape(-1, timesteps, X_test_scaled.shape[1] // timesteps, 1)\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define 3D CNN model\n",
        "cnn_3d_model = models.Sequential([\n",
        "    layers.Conv3D(32, kernel_size=(3, 3, 3), activation=\"relu\", input_shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
        "    layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
        "    layers.Conv3D(64, kernel_size=(3, 3, 3), activation=\"relu\"),\n",
        "    layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_3d_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Train the model\n",
        "history = cnn_3d_model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)\n",
        "\n",
        "# Validate the model\n",
        "y_val_pred = cnn_3d_model.predict(X_val).flatten()\n",
        "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
        "print(f\"3D CNN Validation RMSE: {rmse}\")\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = cnn_3d_model.predict(X_test_reshaped).flatten()\n",
        "\n",
        "# Prepare submission file\n",
        "test_IDs = test_data[\"ID_Zindi\"]\n",
        "submission_predictions = pd.DataFrame({\n",
        "    \"ID_Zindi\": test_IDs,\n",
        "    \"GT_NO2\": test_predictions\n",
        "})\n",
        "\n",
        "# Load all captured IDs and merge\n",
        "all_ids = pd.read_csv(\"all_ids.csv\")\n",
        "submission_full = pd.merge(all_ids, submission_predictions, on=\"ID_Zindi\", how=\"left\")\n",
        "submission_full[\"GT_NO2\"].fillna(submission_full[\"GT_NO2\"].mean(), inplace=True)\n",
        "\n",
        "# Save the final submission file\n",
        "submission_full.to_csv(\"submission_3dcnn.csv\", index=False)\n",
        "print(\"Final 3D CNN submission file created as 'submission_3dcnn.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "Bvh3k_ScfblM",
        "outputId": "38e932f6-2c07-464d-c76e-36b470855be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 909129 into shape (10,1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9955b1429075>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Reshape data for 3D CNN (samples, timesteps, features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m  \u001b[0;31m# Example: use 10 timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mX_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mX_test_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 909129 into shape (10,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kfzFlL9ieWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}